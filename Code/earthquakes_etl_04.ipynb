{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "from sklearn.tree import DecisionTreeClassifier\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "from sklearn.svm import SVC\r\n",
    "from sklearn.svm import LinearSVC\r\n",
    "from sklearn.tree import plot_tree\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "from sklearn.metrics import confusion_matrix, classification_report\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "from sklearn.ensemble import ExtraTreesClassifier\r\n",
    "\r\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Consulta general del dataframe\r\n",
    "def display_all(df):\r\n",
    "    with pd.option_context(\"display.max_rows\",1000 ,  \"display.max_columns\", 1000): \r\n",
    "        display(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Función para identificar el porcentaje de valores nulos por columna\r\n",
    "def percentage_nulls(df):\r\n",
    "    number_nulls = pd.DataFrame(df.isnull().sum(),columns=['Total'])\r\n",
    "    number_nulls['% nulls'] = round((number_nulls['Total'] / df.shape[0])*100,1)\r\n",
    "    \r\n",
    "    return number_nulls"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#Etiquetas para las categorías de decesos, lesionados y daños\r\n",
    "categorias = [0,1,2,3,4]\r\n",
    "etiquetasM = ['Ninguno','entre 1 y 50 decesos','de 51 a 100 decesos','de 101 a 1000 decesos','mas de 1000 decesos']\r\n",
    "etiquetasL = ['Ningun lesionado','entre 1 y 50 lesionados','de 51 a 100 lesionados','de 101 a 1000 lesionados','mas de 1000 lesionados']\r\n",
    "etiquetasD = ['Nada','menos de $1 millón','de $1 a $5 millones','de $5 a $25 millones','mas de $25 millones']\r\n",
    "df_etiquetas_cat = pd.DataFrame()\r\n",
    "df_etiquetas_cat['Categoria'] = categorias\r\n",
    "df_etiquetas_cat['Descesos'] = etiquetasM\r\n",
    "df_etiquetas_cat['Lesionados'] = etiquetasL\r\n",
    "df_etiquetas_cat['Danios'] = etiquetasD"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Función para generar contador de ciudades afectadas por el temblor\r\n",
    "def contar_ciudades(location):\r\n",
    "    paises = location.split(';')\r\n",
    "    ciudades = [c.split(':') for c in paises]\r\n",
    "    contador = 0 \r\n",
    "    if len(ciudades) == 1:\r\n",
    "        contador = len(ciudades[0][-1].split(','))\r\n",
    "    else:\r\n",
    "        for c in ciudades:\r\n",
    "            contador += len(c[-1].split(','))\r\n",
    "    \r\n",
    "    return contador"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# df_earthquakes_work.rename(columns = {'Focal Depth (km)':'Depth','Mag':'Magnitud','MMI Int':'MMI','Death Description':'Death_Cat','Injuries Description':'Injuries_Cat','Damage Description':'Damage_Cat'}, inplace=True)\r\n",
    "# Funcion que renombra columnas\r\n",
    "def rename_columns(df):\r\n",
    "    columns_array = {'Mo':'Month','Dy':'Day','Focal Depth (km)':'Depth','Mag':'Magnitud','MMI Int':'MMI','Death Description':'Death_Cat','Injuries Description':'Injuries_Cat','Damage Description':'Damage_Cat','Houses Destroyed Description':'Houses_Cat'}\r\n",
    "    df.rename(columns=columns_array, inplace = True)\r\n",
    "    return df\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Función para rellenar NULOS de las columnas categoricas y la profundidad con la moda\r\n",
    "def fill_nulls(df):\r\n",
    "    # Se imputan valores con el promedio por Region \r\n",
    "    columnasNull = ['Depth','Magnitud','Density','IDH']\r\n",
    "    for col in columnasNull:\r\n",
    "        region = df[df[col].isnull()]['Region'].array\r\n",
    "        for reg in region:\r\n",
    "            val = df[df['Region'] == reg][col].mean()\r\n",
    "            df.loc[(df['Region'] == reg) & (df[col].isnull()), col] = val\r\n",
    "\r\n",
    "    # data[data['Income($)'].notna()]\r\n",
    "    df = df[(df['Death_Cat'].notna()) & (df['Injuries_Cat'].notna()) & (df['Damage_Cat'])]\r\n",
    "    # df['Death_Cat'].fillna(0, inplace=True)\r\n",
    "    # df['Injuries_Cat'].fillna(0, inplace=True)\r\n",
    "    # df['Damage_Cat'].fillna(0, inplace=True)\r\n",
    "\r\n",
    "    if df['Region'].isnull().sum() > 0:\r\n",
    "        country = df[df['Region'].isnull()]['Country'].unique()\r\n",
    "        for cnt in country:\r\n",
    "            df['Region'].fillna(df[df['Country'] == cnt]['Region'].max(), inplace = True)\r\n",
    "    \r\n",
    "    df[['Month','Day','Region','Death_Cat', 'Injuries_Cat', 'Damage_Cat']] = df[['Month','Day','Region','Death_Cat', 'Injuries_Cat', 'Damage_Cat']].astype('int64')\r\n",
    "    return df\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Seleccion de columnas y registros\r\n",
    "def filtra_df(df, axo=1930):\r\n",
    "    df = df[(df['Year'] >= axo) & (df['Magnitud'] > 0)][['Year', 'Month', 'Day', 'Country', 'Region', 'Location Name', 'Latitude', 'Longitude', 'Depth', 'Magnitud', 'Death_Cat', 'Injuries_Cat', 'Damage_Cat','Density','IDH']]\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Generar columna con contador de ciudades afectadas por el sismo\r\n",
    "def categoriza_ciudad(df):\r\n",
    "    df['Ciudades'] = df['Location Name'].apply(contar_ciudades)\r\n",
    "    df.drop(columns='Location Name', inplace=True)\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Preparar los conjuntos para entrenar los modelos\r\n",
    "#'Latitude','Longitude', 'Ciudades'\r\n",
    "def dataset_train(df):\r\n",
    "    df = df[df['Death_Cat']>0][['Year','Country','Latitude','Longitude','Region','Depth','Magnitud','Death_Cat','Injuries_Cat','Damage_Cat','Density','IDH']]\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def dataframe_transfor(df):\r\n",
    "    #Renombrar columnas\r\n",
    "    df = rename_columns(df)\r\n",
    "    # Filtra y selecciona columnas\r\n",
    "    df = filtra_df(df, 1960)\r\n",
    "    # Crear columna con numero de Ciudades Afectadas\r\n",
    "    df = categoriza_ciudad(df)\r\n",
    "    #Tratamiento de NULOS;\r\n",
    "    df = fill_nulls(df)\r\n",
    "    # df.to_csv('..\\Output\\earthquaqkes_clean.csv')\r\n",
    "    return df\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Asigna los valores de densidad de poblacion e indice de desarrollo humano por pais y año\r\n",
    "def asigna_sociodemo(df, df_sd):\r\n",
    "    df['Density'] = 0\r\n",
    "    df['IDH'] = 0\r\n",
    "\r\n",
    "    for i in df_sd.index:\r\n",
    "        axo = df_sd['Year'][i]\r\n",
    "        country = df_sd['Country'][i]\r\n",
    "        density = df_sd['Density'][i]\r\n",
    "        idh = df_sd['IDH'][i]\r\n",
    "        \r\n",
    "        df.loc[(df['Year'] == axo) & (df['Country'] == country), 'Density'] = density\r\n",
    "        df.loc[(df['Year'] == axo) & (df['Country'] == country), 'IDH'] = idh\r\n",
    "    # df = df[df['Density'] > 0]\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(8, 10))\r\n",
    "\r\n",
    "sns.distplot(\r\n",
    "    df_earthquakes.Deaths,\r\n",
    "    hist    = False,\r\n",
    "    rug     = True,\r\n",
    "    color   = \"blue\",\r\n",
    "    kde_kws = {'shade': True, 'linewidth': 1},\r\n",
    "    ax      = axes[0]\r\n",
    ")\r\n",
    "axes[0].set_title(\"Distribución original\", fontsize = 'medium')\r\n",
    "axes[0].set_xlabel('Decesos', fontsize='small') \r\n",
    "axes[0].tick_params(labelsize = 6)\r\n",
    "\r\n",
    "sns.distplot(\r\n",
    "    np.sqrt(df_earthquakes.Deaths),\r\n",
    "    hist    = False,\r\n",
    "    rug     = True,\r\n",
    "    color   = \"blue\",\r\n",
    "    kde_kws = {'shade': True, 'linewidth': 1},\r\n",
    "    ax      = axes[1]\r\n",
    ")\r\n",
    "axes[1].set_title(\"Transformación raíz cuadrada\", fontsize = 'medium')\r\n",
    "axes[1].set_xlabel('sqrt(Decesos)', fontsize='small') \r\n",
    "axes[1].tick_params(labelsize = 6)\r\n",
    "\r\n",
    "sns.distplot(\r\n",
    "    np.log(df_earthquakes.Deaths),\r\n",
    "    hist    = False,\r\n",
    "    rug     = True,\r\n",
    "    color   = \"blue\",\r\n",
    "    kde_kws = {'shade': True, 'linewidth': 1},\r\n",
    "    ax      = axes[2]\r\n",
    ")\r\n",
    "axes[2].set_title(\"Transformación logarítmica\", fontsize = 'medium')\r\n",
    "axes[2].set_xlabel('log(Decesos)', fontsize='small') \r\n",
    "axes[2].tick_params(labelsize = 6)\r\n",
    "\r\n",
    "fig.tight_layout()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Gráfico de distribución para cada variable numérica\r\n",
    "# ==============================================================================\r\n",
    "# Ajustar número de subplots en función del número de columnas\r\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(9, 5))\r\n",
    "axes = axes.flat\r\n",
    "columnas_numeric = df_earthquakes.select_dtypes(include=['float64', 'int']).columns\r\n",
    "columnas_numeric = columnas_numeric.drop(['Mo', 'Dy', 'MMI Int', 'Deaths', 'Death Description','Injuries Description', 'Damage Description','Houses Destroyed', 'Houses Destroyed Description', 'Houses Damaged','Houses Damaged Description'])\r\n",
    "columnas_numeric\r\n",
    "\r\n",
    "for i, colum in enumerate(columnas_numeric):\r\n",
    "    sns.histplot(\r\n",
    "        data    = df_earthquakes,\r\n",
    "        x       = colum,\r\n",
    "        stat    = \"count\",\r\n",
    "        kde     = True,\r\n",
    "        color   = (list(plt.rcParams['axes.prop_cycle'])*2)[i][\"color\"],\r\n",
    "        line_kws= {'linewidth': 2},\r\n",
    "        alpha   = 0.3,\r\n",
    "        ax      = axes[i]\r\n",
    "    )\r\n",
    "    axes[i].set_title(colum, fontsize = 7, fontweight = \"bold\")\r\n",
    "    axes[i].tick_params(labelsize = 6)\r\n",
    "    axes[i].set_xlabel(\"\")\r\n",
    "    \r\n",
    "    \r\n",
    "fig.tight_layout()\r\n",
    "plt.subplots_adjust(top = 0.9)\r\n",
    "fig.suptitle('Distribución variables numéricas', fontsize = 10, fontweight = \"bold\");"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Extraccion datos fuente\r\n",
    "df_earthquakes = pd.read_csv('..\\Dataset\\earthquakes-1500-2021.csv')\r\n",
    "df_sociodemo = pd.read_csv('..\\Dataset\\socio_demografico.csv')\r\n",
    "# Asocia valores de densidad de poblacion e idh\r\n",
    "df_earthquakes = asigna_sociodemo(df_earthquakes, df_sociodemo)\r\n",
    "# Transformación \r\n",
    "df_earthquakes = dataframe_transfor(df_earthquakes)\r\n",
    "# Se generan datos para cada target\r\n",
    "df_earthquakes_train = dataset_train(df_earthquakes)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3065: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(12,8))\r\n",
    "sns.heatmap(df_earthquakes_train.drop(columns=['Death_Cat']).corr(), annot=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(12,8))\r\n",
    "sns.heatmap(df_earthquakes_train.drop(columns=['Injuries_Cat']).corr(), annot=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(12,8))\r\n",
    "sns.heatmap(df_earthquakes_train.drop(columns=['Damage_Cat']).corr(), annot=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Proceso de modelado Decesos"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# Utilizando Arboles de decisión para validar si da mejores resultados que RegressionLogistics\r\n",
    "def modelo_dt_deahts(df):\r\n",
    "    # Separar los conjuntos (Target y variables predictoras)\r\n",
    "    X = df.drop(columns = ['Death_Cat','Injuries_Cat','Damage_Cat','Country','Latitude','Longitude'])\r\n",
    "    y = df['Death_Cat']\r\n",
    "\r\n",
    "    #Balanceando los conjuntos\r\n",
    "    ros = RandomOverSampler()\r\n",
    "    X_ros, y = ros.fit_resample(X, y)\r\n",
    "\r\n",
    "    #Escalando datos\r\n",
    "    scaler = StandardScaler()\r\n",
    "    X_t = scaler.fit_transform(X_ros)\r\n",
    "\r\n",
    "    #Creando set de entrenamiento y prueba\r\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_t, y, test_size = 0.2, shuffle=True, random_state=50) \r\n",
    "    # asdf\r\n",
    "    modelo_DT = DecisionTreeClassifier(max_depth = 8, criterion = 'gini', random_state = 123)\r\n",
    "    modelo_DT.fit(X_train, y_train)\r\n",
    "    return modelo_DT, X_test, y_test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "modelo_deaths, X_test, y_test = modelo_dt_deahts(df_earthquakes_train)\r\n",
    "y_predict_deaths = modelo_deaths.predict(X_test)\r\n",
    "df_pred_Deaths_DT = pd.DataFrame({'Actual':y_test,'Predicted':y_predict_deaths})\r\n",
    "# df_pred_Deaths_DT.head()\r\n",
    "precision = round(modelo_deaths.score(X_test, y_test) *100,1)\r\n",
    "precision"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "75.8"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "print(classification_report(y_test, y_predict_deaths))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.61      0.76       136\n",
      "           2       0.75      0.83      0.79       104\n",
      "           3       0.70      0.74      0.72       127\n",
      "           4       0.69      0.88      0.77       121\n",
      "\n",
      "    accuracy                           0.76       488\n",
      "   macro avg       0.78      0.77      0.76       488\n",
      "weighted avg       0.79      0.76      0.76       488\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "df_deaths_grf = pd.DataFrame()\r\n",
    "df_deaths_grf['Test'] = y_test\r\n",
    "df_deaths_grf['Pred'] = y_predict_deaths\r\n",
    "df_deaths_grf.reset_index(drop=True)\r\n",
    "df_deaths_grf.head()\r\n",
    "df_deaths_grf.to_csv('../Output/earthquaks_death_predicted.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modelado para Lesionados"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "def modelo_dt_injuries(df):\r\n",
    "    # Separar los conjuntos (Target y variables predictoras)\r\n",
    "    X = df.drop(columns = ['Injuries_Cat','Damage_Cat','Country','Latitude','Longitude'])\r\n",
    "    y = df['Injuries_Cat']\r\n",
    "    \r\n",
    "    #Balanceo de datos\r\n",
    "    ros = RandomOverSampler()\r\n",
    "    X_ros, y = ros.fit_resample(X, y)\r\n",
    "\r\n",
    "    #Escalando datos\r\n",
    "    scaler = StandardScaler()\r\n",
    "    X_t = scaler.fit_transform(X_ros)\r\n",
    "\r\n",
    "    #Creando set de entrenamiento y prueba\r\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_t, y, test_size = 0.2, shuffle=True, random_state=50) \r\n",
    "    # asdf\r\n",
    "    modelo = DecisionTreeClassifier(criterion = 'gini', random_state = 123)\r\n",
    "    modelo.fit(X_train, y_train)\r\n",
    "\r\n",
    "    return modelo, X_test, y_test \r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "modelo_injuries, X_test, y_test = modelo_dt_injuries(df_earthquakes_train) #modelo_dt_injuries(df_earthquakes_injuries)\r\n",
    "y_predict_injuries = modelo_injuries.predict(X_test)\r\n",
    "df_pred_injuries_DT = pd.DataFrame({'Actual':y_test,'Predicted':y_predict_injuries})\r\n",
    "precision = round(modelo_injuries.score(X_test, y_test) *100,1)\r\n",
    "precision"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "75.9"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "print(classification_report(y_test, y_predict_injuries))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.68      0.61      0.64        75\n",
      "           2       0.78      0.91      0.84        56\n",
      "           3       0.61      0.56      0.58        61\n",
      "           4       0.94      0.98      0.96        65\n",
      "\n",
      "    accuracy                           0.76       257\n",
      "   macro avg       0.75      0.77      0.76       257\n",
      "weighted avg       0.75      0.76      0.75       257\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "df_injuries_grf = pd.DataFrame()\r\n",
    "df_injuries_grf['Test'] = y_test\r\n",
    "df_injuries_grf['Pred'] = y_predict_injuries\r\n",
    "df_injuries_grf.reset_index(drop=True)\r\n",
    "df_injuries_grf.head()\r\n",
    "df_injuries_grf.to_csv('../Output/earthquaks_injuries_predicted.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Proceso de Modelado Daños"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "df = df_earthquakes_train.copy()\r\n",
    "\r\n",
    "# Separar los conjuntos (Target y variables predictoras)\r\n",
    "X = df.drop(columns = ['Damage_Cat','Country','Latitude','Longitude'])\r\n",
    "y = df['Damage_Cat']\r\n",
    "\r\n",
    "# #Balanceo de datos\r\n",
    "ros = RandomOverSampler()\r\n",
    "X_ros, y = ros.fit_resample(X, y)\r\n",
    "\r\n",
    "# #Escalando datos\r\n",
    "scaler = StandardScaler()\r\n",
    "X_t = scaler.fit_transform(X_ros)\r\n",
    "\r\n",
    "# #Creando set de entrenamiento y prueba\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_t, y, test_size = 0.2, shuffle=True, random_state=50) \r\n",
    "\r\n",
    "# #Entrenamiento\r\n",
    "# modelo = DecisionTreeClassifier(criterion = 'gini', min_samples_split=20, min_samples_leaf=5, random_state = 30)\r\n",
    "#n_estimators=100, criterion='gini', max_depth=3, min_samples_split=4, min_samples_leaf=2,bootstrap=True\r\n",
    "modelo_damage = RandomForestClassifier(n_estimators=600, criterion='gini')\r\n",
    "# modelo = LogisticRegression(solver='newton-cg',penalty='l2', multi_class='ovr')\r\n",
    "# modelo = SVC(decision_function_shape='ovo')\r\n",
    "# modelo = LinearSVC()\r\n",
    "\r\n",
    "modelo_damage.fit(X_train, y_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=600)"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "y_pred = modelo_damage.predict(X_test)\r\n",
    "# df_earthquakes_train.head()\r\n",
    "\r\n",
    "matriz = confusion_matrix(y_test, y_pred)\r\n",
    "print(matriz)\r\n",
    "print('\\n')\r\n",
    "exactitud = accuracy_score(y_test, y_pred, normalize=True)\r\n",
    "print(round(exactitud*100,1), '%')\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[62  6  4  0]\n",
      " [ 9 29 12  3]\n",
      " [ 2  5 34  7]\n",
      " [ 0  4  7 45]]\n",
      "\n",
      "\n",
      "74.2 %\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "print(classification_report(y_test, y_pred))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.85      0.86      0.86        72\n",
      "           2       0.66      0.55      0.60        53\n",
      "           3       0.60      0.71      0.65        48\n",
      "           4       0.82      0.80      0.81        56\n",
      "\n",
      "    accuracy                           0.74       229\n",
      "   macro avg       0.73      0.73      0.73       229\n",
      "weighted avg       0.74      0.74      0.74       229\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "df_damage_grf = pd.DataFrame()\r\n",
    "df_damage_grf['Test'] = y_test\r\n",
    "df_damage_grf['Pred'] = y_pred\r\n",
    "df_damage_grf.reset_index(drop=True)\r\n",
    "df_damage_grf.head()\r\n",
    "df_damage_grf.to_csv('../Output/earthquaks_damage_predicted.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "df_prueba_gpo = pd.DataFrame(df_damage_grf['Test'].value_counts().reset_index())\r\n",
    "# df_prueba_gpo = pd.DataFrame(df_damage_grf['Pred'].value_counts())\r\n",
    "df_prueba_gpo.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  Test\n",
       "0      1    72\n",
       "1      4    56\n",
       "2      2    53\n",
       "3      3    48"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Dataset para modelado final PRODUCTIVO\r\n",
    "df_eq_modelado = df_earthquakes.drop(columns=['Month','Day','Death_Cat','Injuries_Cat','Damage_Cat',]).copy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predicción de Muertos para el Dataset FINAL"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Transformar los datos\r\n",
    "scaler = StandardScaler()\r\n",
    "X_final = scaler.fit_transform(df_eq_modelado[['Year','Region','Depth','Magnitud','Density','IDH']])\r\n",
    "\r\n",
    "# Obtener la prediccion de los Decesos con la información de los lesionados\r\n",
    "predict_deaths = modelo_deaths.predict(X_final)\r\n",
    "df_eq_modelado['Death_Cat'] = predict_deaths\r\n",
    "df_eq_modelado.head(3)\r\n",
    "# df_final.to_csv('../Output/earthquaks_model_deaths.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predicción de Lesionados para el dataset Final"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Transformar los datos\r\n",
    "scaler = StandardScaler()\r\n",
    "X_final = scaler.fit_transform(df_eq_modelado[['Year','Region','Depth','Magnitud','Density','IDH','Death_Cat']])\r\n",
    "\r\n",
    "# Obtener la prediccion de los lesionados con la información \r\n",
    "predict_injuries = modelo_injuries.predict(X_final)\r\n",
    "df_eq_modelado['Injuries_Cat'] = predict_injuries\r\n",
    "df_eq_modelado.head(3)\r\n",
    "# df_final.to_csv('../Output/earthquaks_model_injuries.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predicción de Daños para el dataset Final"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Transformar los datos\r\n",
    "scaler = StandardScaler()\r\n",
    "X_final = scaler.fit_transform(df_eq_modelado[['Year','Region','Depth','Magnitud','Density','IDH','Death_Cat','Injuries_Cat']])\r\n",
    "\r\n",
    "# Obtener la prediccion de los Decesos con la información de los lesionados\r\n",
    "predict_damage = modelo_damage.predict(X_final)\r\n",
    "df_eq_modelado['Damage_Cat'] = predict_damage\r\n",
    "df_eq_modelado.head(3)\r\n",
    "\r\n",
    "# Genera los grupos para la densidad de población\r\n",
    "# ALPHA para que los extremos no coincidan con los valores de los datos\r\n",
    "alpha = 1\r\n",
    "bins = np.linspace(df_eq_modelado.Density.min()- alpha,df_eq_modelado.Density.max() + alpha,5)\r\n",
    "i = np.digitize(df_eq_modelado.Density,bins)\r\n",
    "df_eq_modelado['Density_Gpo'] = i\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Agrega descripción de etiquetas\r\n",
    "for i in df_etiquetas_cat.index:\r\n",
    "    df_eq_modelado.loc[df_eq_modelado['Death_Cat'] == df_etiquetas_cat['Categoria'][i], 'Death_Descripcion'] = df_etiquetas_cat['Descesos'][i]#######\r\n",
    "    df_eq_modelado.loc[df_eq_modelado['Injuries_Cat'] == df_etiquetas_cat['Categoria'][i], 'Injurie_Descripcion'] = df_etiquetas_cat['Lesionados'][i]\r\n",
    "    df_eq_modelado.loc[df_eq_modelado['Damage_Cat'] == df_etiquetas_cat['Categoria'][i], 'Damage_Descripcion'] = df_etiquetas_cat['Danios'][i]\r\n",
    "\r\n",
    "# Se genera el archivo para realizar las visualizaciones en Power BI\r\n",
    "df_eq_modelado.to_csv('../Output/earthquaks_modelado_did.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_idh_bajo_death_1 = df_eq_modelado[(df_eq_modelado.Year >= 2010) & (df_eq_modelado['IDH'] >= 0.56) & (df_eq_modelado['IDH'] < 0.68)  & (df_eq_modelado['Death_Cat'] == 1)]['IDH'].count()\r\n",
    "df_idh_medio_death_1 = df_eq_modelado[(df_eq_modelado.Year >= 2010) & (df_eq_modelado['IDH'] >= 0.68) & (df_eq_modelado['IDH'] < 0.80) & (df_eq_modelado['Death_Cat'] == 1)]['IDH'].count()\r\n",
    "df_idh_alto_death_1 = df_eq_modelado[(df_eq_modelado.Year >= 2010) & (df_eq_modelado['IDH'] >= 0.80) & (df_eq_modelado['Death_Cat'] == 1)]['IDH'].count()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bins"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Genera los grupos para la densidad de población\r\n",
    "# ALPHA para que los extremos no coincidan con los valores de los datos\r\n",
    "alpha = 1\r\n",
    "bins = np.linspace(df_final.Density.min()- alpha,df_final.Density.max() + alpha,5)\r\n",
    "i = np.digitize(df_final.Density,bins)\r\n",
    "df_final['Density_Gpo'] = i\r\n",
    "\r\n",
    "# Agrega descripción de etiquetas\r\n",
    "for i in df_etiquetas_cat.index:\r\n",
    "    df_final.loc[df_final['Death_Cat'] == df_etiquetas_cat['Categoria'][i], 'Death_Descripcion'] = df_etiquetas_cat['Descesos'][i]#######\r\n",
    "    df_final.loc[df_final['Injuries_Cat'] == df_etiquetas_cat['Categoria'][i], 'Injurie_Descripcion'] = df_etiquetas_cat['Lesionados'][i]\r\n",
    "    df_final.loc[df_final['Damage_Cat'] == df_etiquetas_cat['Categoria'][i], 'Damage_Descripcion'] = df_etiquetas_cat['Danios'][i]\r\n",
    "\r\n",
    "# Se genera el archivo para realizar las visualizaciones en Power BI\r\n",
    "df_final.to_csv('../Output/earthquaks_final_model.csv', index=False)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fin modelado de Decesos, Lesionados y Daños materiales"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Se crea catalogo de regiones para agregar al dataset final\r\n",
    "df_cat_regions = df_earthquakes.groupby(['Country','Region']).agg({'Location Name':'count'}).reset_index()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Leer archivo para predicción de dataset final\r\n",
    "df_final = pd.read_csv('..\\Output\\earthquaqkes_all_clean.csv')\r\n",
    "df_final = df_final[['Year', 'Date','Latitude','Longitude','Country','Depth','Magnitud' ]]\r\n",
    "#Pegar los datos de densidad de población e indice de desarrollo\r\n",
    "df_final = asigna_sociodemo(df_final, df_sociodemo)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Asociar catalogo de regiones con el dataset final\r\n",
    "df_final['Region'] = 0\r\n",
    "\r\n",
    "for i in df_cat_regions.index:\r\n",
    "    df_final.loc[(df_final['Country'] == df_cat_regions['Country'][i]), 'Region'] = df_cat_regions['Region'][i]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Agrega descripción de etiquetas\r\n",
    "for i in df_etiquetas_cat.index:\r\n",
    "    df_final.loc[df_final['Death_Cat'] == df_etiquetas_cat['Categoria'][i], 'Death_Descripcion'] = df_etiquetas_cat['Descesos'][i]#######\r\n",
    "    df_final.loc[df_final['Injuries_Cat'] == df_etiquetas_cat['Categoria'][i], 'Injurie_Descripcion'] = df_etiquetas_cat['Lesionados'][i]\r\n",
    "    df_final.loc[df_final['Damage_Cat'] == df_etiquetas_cat['Categoria'][i], 'Damage_Descripcion'] = df_etiquetas_cat['Danios'][i]\r\n",
    "\r\n",
    "# Se genera el archivo para realizar las visualizaciones en Power BI\r\n",
    "df_final.to_csv('../Output/earthquaks_final_model.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Obtener el TOP 10 de más temblores por año\r\n",
    "df_top_earthquakesBy_YearCountry = df_final[(df_final['Country'] != 'NO_COUNTRY') & (df_final['Density'] > 0)].groupby(['Year','Country']).agg({'Depth':'count'}).reset_index()\r\n",
    "df_top_earthquakesBy_YearCountry.rename(columns={'Depth':'Earthquakes'}, inplace=True)\r\n",
    "\r\n",
    "# Se arma el Top 10 de paises con más temblores por año\r\n",
    "years = list(df_top_earthquakesBy_YearCountry.Year.unique())\r\n",
    "df_Top = pd.DataFrame()\r\n",
    "for y in years:\r\n",
    "    df_Top = df_Top.append(df_top_earthquakesBy_YearCountry[df_top_earthquakesBy_YearCountry['Year'] == y].nlargest(10,'Earthquakes'), ignore_index=True)\r\n",
    "\r\n",
    "# Genera el archivo para Power BI\r\n",
    "df_Top.to_csv('../Output/earthquaks_top_ten.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "modelo_injuries, X_test, y_test = modelo_dt_injuries(df_earthquakes_train) #modelo_dt_injuries(df_earthquakes_injuries)\r\n",
    "y_predict_injuries = modelo_injuries.predict(X_test)\r\n",
    "df_pred_injuries_DT = pd.DataFrame({'Actual':y_test,'Predicted':y_predict_injuries})\r\n",
    "precision = round(modelo_injuries.score(X_test, y_test) *100,1)\r\n",
    "precision"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_earthquakes_injuries.head(1)\r\n",
    "# Region\tDepth\tMagnitud\tInjuries_Cat\tCiudades\r\n",
    "# Year\tMonth\tDay\tCountry Latitude\tLongitude Death_Cat\tInjuries_Cat\tDamage_Cat\tDensity\tCiudades"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_earthquakes_damage.Damage_Cat.value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "modelo.score(X_test, y_test)\r\n",
    "# modelo.n_classes_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from collections import Counter\r\n",
    "Counter(y).items()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Podado de arbol para identificar el mejor nivel por validación cruzada\r\n",
    "# ------------------------------------------------------------------------------\r\n",
    "# Valores de ccp_alpha evaluados\r\n",
    "param_grid = {'ccp_alpha':np.linspace(0, 5, 10)}\r\n",
    "\r\n",
    "# Búsqueda por validación cruzada\r\n",
    "grid = GridSearchCV(\r\n",
    "        # El árbol se crece al máximo posible antes de aplicar el pruning\r\n",
    "        estimator = DecisionTreeClassifier(\r\n",
    "                            criterion         = 'entropy',\r\n",
    "                            min_samples_split = 20,\r\n",
    "                            min_samples_leaf  = 5,\r\n",
    "                            random_state      = 30),\r\n",
    "        param_grid = param_grid,\r\n",
    "        scoring    = 'accuracy',\r\n",
    "        cv         = 10,\r\n",
    "        refit      = True,\r\n",
    "        return_train_score = True)\r\n",
    "\r\n",
    "grid.fit(X_train, y_train)\r\n",
    "\r\n",
    "# fig, ax = plt.subplots(figsize=(6, 3.84))\r\n",
    "# scores = pd.DataFrame(grid.cv_results_)\r\n",
    "# scores.plot(x='param_ccp_alpha', y='mean_train_score', yerr='std_train_score', ax=ax)\r\n",
    "# scores.plot(x='param_ccp_alpha', y='mean_test_score', yerr='std_test_score', ax=ax)\r\n",
    "# ax.set_title(\"Error de validacion cruzada vs hiperparámetro ccp_alpha\");"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grid.best_params_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "modelo_final_injuries = grid.best_estimator_\r\n",
    "print(f\"Profundidad del árbol: {modelo_final_injuries.get_depth()}\")\r\n",
    "print(f\"Número de nodos terminales: {modelo_final_injuries.get_n_leaves()}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predicciones = modelo_final_injuries.predict(X = X_test)\r\n",
    "accuracy = accuracy_score(y_true= y_test, y_pred= predicciones,normalize = True)\r\n",
    "print(f\"El accuracy de test es: {100 * accuracy} %\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "labels = X.columns.to_list()\r\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\r\n",
    "\r\n",
    "print(f\"Profundidad del árbol: {modelo.get_depth()}\")\r\n",
    "print(f\"Número de nodos terminales: {modelo.get_n_leaves()}\")\r\n",
    "\r\n",
    "plot = plot_tree( decision_tree = modelo, feature_names = labels, class_names = 'deaths_category', filled = True, \r\n",
    "impurity= False, fontsize= 7, ax = ax)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Importancia de los predictores en el modelo\")\r\n",
    "print(\"-------------------------------------------\")\r\n",
    "importancia_predictores = pd.DataFrame({'predictor': labels,'importancia': modelo_final.feature_importances_})\r\n",
    "importancia_predictores.sort_values('importancia', ascending=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Selección de características mediante Arbol de clasificación (Grafica)\r\n",
    "tree_classifier = ExtraTreesClassifier(n_estimators=100, criterion='gini')\r\n",
    "tree_classifier.fit(X_t,y)\r\n",
    "index = np.flipud(np.argsort(tree_classifier.feature_importances_))\r\n",
    "score = tree_classifier.feature_importances_[index]\r\n",
    "\r\n",
    "fig,ax = plt.subplots(figsize=(20,8))\r\n",
    "plt.bar(range(len(index)),score,align='center')\r\n",
    "plt.xticks(range(len(index)),index)\r\n",
    "plt.title('importances of features')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_predict_DT = modelo_DT.predict(X_test)\r\n",
    "df_pred_Deaths_DT = pd.DataFrame({'Actual':y_test,'Predicted':y_predict_DT})\r\n",
    "df_pred_Deaths_DT['Validacion'] = abs(df_pred_Deaths_DT['Actual'] - df_pred_Deaths_DT['Predicted'])\r\n",
    "correctos = df_pred_Deaths_DT[df_pred_Deaths_DT['Validacion'] == 0]['Validacion'].count()\r\n",
    "total = df_pred_Deaths_DT.shape[0]\r\n",
    "incorrectos = total - correctos\r\n",
    "porcentaje = (correctos / total) * 100\r\n",
    "print(f'Correctos: {correctos}')\r\n",
    "print(f'Incorrectos: {incorrectos}')\r\n",
    "print(f'Precision: {porcentaje}')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "confusion_matrix(y_test, y_predict_DT)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "accuracy_DT = accuracy_score(y_true=y_test, y_pred=y_predict_DT, normalize=True)\r\n",
    "print(f'El accuracy test es de: {accuracy_DT * 100} %')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}